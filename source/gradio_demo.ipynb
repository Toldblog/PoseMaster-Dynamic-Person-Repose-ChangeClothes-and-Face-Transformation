{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ea36acf91ce249e2b27434e2072f4b30":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_421392369d6840bc9dc1c9da6a73ee96","IPY_MODEL_61b67ef9069f4d569d5cf3f03051d22f","IPY_MODEL_45e53a0d0b044ad8abedf266f52735ac"],"layout":"IPY_MODEL_2eb6900887ce4e67a511587a6d4ee6e7"}},"421392369d6840bc9dc1c9da6a73ee96":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3d755fc5c224bdfa8edf1d37883edb2","placeholder":"​","style":"IPY_MODEL_5eb816407d594d1fb7d800510b542ca5","value":"100%"}},"61b67ef9069f4d569d5cf3f03051d22f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b56e9c3c99b4290a98247d9aa999450","max":15,"min":0,"orientation":"horizontal","style":"IPY_MODEL_859b503102bd44aa9fdf1490393d1d34","value":15}},"45e53a0d0b044ad8abedf266f52735ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f959834e20474bf4b7629b893fbdb024","placeholder":"​","style":"IPY_MODEL_314787ae608546c4b1d95e0bf655a524","value":" 15/15 [00:22&lt;00:00,  1.15s/it]"}},"2eb6900887ce4e67a511587a6d4ee6e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3d755fc5c224bdfa8edf1d37883edb2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5eb816407d594d1fb7d800510b542ca5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1b56e9c3c99b4290a98247d9aa999450":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"859b503102bd44aa9fdf1490393d1d34":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f959834e20474bf4b7629b893fbdb024":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"314787ae608546c4b1d95e0bf655a524":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8a236713591141a2a585265b945f1120":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_96045f66b8ad412ebac5189e30ef337b","IPY_MODEL_d34640a2b79d4215b3eb04e58463e013","IPY_MODEL_19540eaab9584a94b126aeb3ffb8c0d7"],"layout":"IPY_MODEL_621e43a66a414c1784d11dd0e157fc99"}},"96045f66b8ad412ebac5189e30ef337b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_643cc5c7bcca471999a8c9d7bb3e14a6","placeholder":"​","style":"IPY_MODEL_6886ef870ef24bd6b6419cee0df6f0d2","value":"100%"}},"d34640a2b79d4215b3eb04e58463e013":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_93cd5b796989485ab7198e2a9cd17296","max":13,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2512eefc125d4a308948c0c81389bf33","value":13}},"19540eaab9584a94b126aeb3ffb8c0d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca2c15fc95f340199a5808db6cc353c4","placeholder":"​","style":"IPY_MODEL_c267ca2be886453ab4aa0cfcb699718d","value":" 13/13 [00:24&lt;00:00,  1.83s/it]"}},"621e43a66a414c1784d11dd0e157fc99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"643cc5c7bcca471999a8c9d7bb3e14a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6886ef870ef24bd6b6419cee0df6f0d2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93cd5b796989485ab7198e2a9cd17296":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2512eefc125d4a308948c0c81389bf33":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ca2c15fc95f340199a5808db6cc353c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c267ca2be886453ab4aa0cfcb699718d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0394be08d79248188382c08d138d471e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_41abb300f21d43c99ce2ef9170cacd07","IPY_MODEL_26487ec5c8d947e09a78fb90797f8275","IPY_MODEL_b641419419bd4667a808f6e9b7792794"],"layout":"IPY_MODEL_80968e2b75e541bebece82e939827e47"}},"41abb300f21d43c99ce2ef9170cacd07":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8114a0f9971b470a90580ac079e1b0b2","placeholder":"​","style":"IPY_MODEL_4376b003c0e94e28ae5984c12ba7f70a","value":"100%"}},"26487ec5c8d947e09a78fb90797f8275":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6939c645a86a44788a62148f1dc006e0","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eab2c6a995d14c039006480c7cadfc40","value":10}},"b641419419bd4667a808f6e9b7792794":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2efc4055b88b4e70b58815b2319b3a19","placeholder":"​","style":"IPY_MODEL_abf697efbff94eb59529d77c58ebfff0","value":" 10/10 [01:29&lt;00:00,  8.79s/it]"}},"80968e2b75e541bebece82e939827e47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8114a0f9971b470a90580ac079e1b0b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4376b003c0e94e28ae5984c12ba7f70a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6939c645a86a44788a62148f1dc006e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eab2c6a995d14c039006480c7cadfc40":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2efc4055b88b4e70b58815b2319b3a19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abf697efbff94eb59529d77c58ebfff0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"42d014777de043d6880bb86dfb06f962":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_af1f8f4ea20e4b419a970c287379fbc4","IPY_MODEL_8926e3b30d694587a25b014a38bbc3e0","IPY_MODEL_17ee25ca3db94d0b87358a7d2f08741c"],"layout":"IPY_MODEL_882e44afe0b34fbf869a5aa561eaf9da"}},"af1f8f4ea20e4b419a970c287379fbc4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0857450521942df96df5c74184f6929","placeholder":"​","style":"IPY_MODEL_e5064ec8957945c8a2fc50a8da66de11","value":"100%"}},"8926e3b30d694587a25b014a38bbc3e0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_130e8f5d2a3a4a4f979fe900ffb5f830","max":16,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2b6e2fb5ad38420c8e796d61a6cbd6c2","value":16}},"17ee25ca3db94d0b87358a7d2f08741c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_175ab3e5c64d46d79cd17b310a6c93f8","placeholder":"​","style":"IPY_MODEL_8605083047414c1ba01660eeb11a8220","value":" 16/16 [00:07&lt;00:00,  2.36it/s]"}},"882e44afe0b34fbf869a5aa561eaf9da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0857450521942df96df5c74184f6929":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5064ec8957945c8a2fc50a8da66de11":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"130e8f5d2a3a4a4f979fe900ffb5f830":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b6e2fb5ad38420c8e796d61a6cbd6c2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"175ab3e5c64d46d79cd17b310a6c93f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8605083047414c1ba01660eeb11a8220":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rehXzF6UnuND","executionInfo":{"status":"ok","timestamp":1728553608478,"user_tz":-420,"elapsed":4435,"user":{"displayName":"Hung To","userId":"12784896086462809074"}},"outputId":"2eca0b28-d509-4cc9-ff77-a095da425ee1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install diffusers torchsde kornia spandrel lpips black timm addict yapf rembg fastapi asyncio uvicorn nest_asyncio pyngrok python-multipart gradio"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4t9SXRetn2Ts","executionInfo":{"status":"ok","timestamp":1728553613029,"user_tz":-420,"elapsed":4553,"user":{"displayName":"Hung To","userId":"12784896086462809074"}},"outputId":"744183af-5424-4eb4-b0d9-cd128fbdd42a","collapsed":true},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (0.30.3)\n","Requirement already satisfied: torchsde in /usr/local/lib/python3.10/dist-packages (0.2.6)\n","Requirement already satisfied: kornia in /usr/local/lib/python3.10/dist-packages (0.7.3)\n","Requirement already satisfied: spandrel in /usr/local/lib/python3.10/dist-packages (0.4.0)\n","Requirement already satisfied: lpips in /usr/local/lib/python3.10/dist-packages (0.1.4)\n","Requirement already satisfied: black in /usr/local/lib/python3.10/dist-packages (24.10.0)\n","Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.9)\n","Requirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (2.4.0)\n","Requirement already satisfied: yapf in /usr/local/lib/python3.10/dist-packages (0.40.2)\n","Requirement already satisfied: rembg in /usr/local/lib/python3.10/dist-packages (2.0.59)\n","Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (0.115.0)\n","Requirement already satisfied: asyncio in /usr/local/lib/python3.10/dist-packages (3.4.3)\n","Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (0.31.1)\n","Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n","Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.0)\n","Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (0.0.12)\n","Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.0.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (8.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.16.1)\n","Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.25.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.26.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.4.5)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (10.4.0)\n","Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from torchsde) (1.13.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from torchsde) (2.4.1+cu121)\n","Requirement already satisfied: trampoline>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from torchsde) (0.1.2)\n","Requirement already satisfied: kornia-rs>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from kornia) (0.1.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kornia) (24.1)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from spandrel) (0.19.1+cu121)\n","Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from spandrel) (0.8.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from spandrel) (4.12.2)\n","Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (4.66.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black) (8.1.7)\n","Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black) (1.0.0)\n","Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from black) (0.12.1)\n","Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black) (4.3.6)\n","Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black) (2.0.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from rembg) (4.23.0)\n","Requirement already satisfied: onnxruntime in /usr/local/lib/python3.10/dist-packages (from rembg) (1.19.2)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from rembg) (4.10.0.84)\n","Requirement already satisfied: pooch in /usr/local/lib/python3.10/dist-packages (from rembg) (1.8.2)\n","Requirement already satisfied: pymatting in /usr/local/lib/python3.10/dist-packages (from rembg) (1.1.12)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from rembg) (0.24.0)\n","Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.38.6)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.9.2)\n","Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n","Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n","Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n","Requirement already satisfied: gradio-client==1.4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.4.0)\n","Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n","Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.7)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n","Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n","Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.6.9)\n","Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n","Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n","Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.0->gradio) (2024.6.1)\n","Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.0->gradio) (12.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.20.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.23.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->torchsde) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->torchsde) (3.3)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.1)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (24.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (0.20.0)\n","Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime->rembg) (15.0.1)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime->rembg) (24.3.25)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime->rembg) (3.20.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.3.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.2.3)\n","Requirement already satisfied: numba!=0.49.0 in /usr/local/lib/python3.10/dist-packages (from pymatting->rembg) (0.60.0)\n","Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (2.35.1)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (2024.9.20)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (0.4)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba!=0.49.0->pymatting->rembg) (0.43.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n","Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime->rembg) (10.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->torchsde) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"]}]},{"cell_type":"code","source":["# @title Import libraries\n","\n","import os\n","import random\n","import sys\n","from typing import Sequence, Mapping, Any, Union\n","import torch\n","import gc\n","import json\n","from PIL import Image, ImageOps, ImageSequence, ImageFile\n","from PIL.PngImagePlugin import PngInfo\n","import hashlib\n","import numpy as np\n","import asyncio\n","\n","if True:\n","    sys.path.append(\"/content/drive/MyDrive/Thesis/StableDiffusion/ComfyUI/Test/ComfyUI-master\")\n","    from nodes import NODE_CLASS_MAPPINGS, init_extra_nodes\n","    import comfy.model_management as model_management\n","    import folder_paths\n","    import node_helpers\n","    import execution\n","    import server\n","\n","    sys.path.append(\"/content/drive/MyDrive/Thesis/StableDiffusion/ComfyUI/Test/ComfyUI-master/custom_nodes/comfyui_controlnet_aux/src\")\n","    from custom_controlnet_aux.dwpose import DwposeDetector, AnimalposeDetector\n","\n","    sys.path.append(\"/content/drive/MyDrive/Thesis/StableDiffusion/ComfyUI/Test/ComfyUI-master/custom_nodes\")\n","    from comfyui_controlnet_aux.utils import common_annotator_call, define_preprocessor_inputs, INPUT\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tiVKT92Tn6Zd","executionInfo":{"status":"ok","timestamp":1728553629001,"user_tz":-420,"elapsed":15975,"user":{"displayName":"Hung To","userId":"12784896086462809074"}},"outputId":"dffab6bf-d93b-4cf7-c5fe-ffadbd10ac4c","collapsed":true,"cellView":"form"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/Thesis/StableDiffusion/ComfyUI/Test/ComfyUI-master/custom_nodes/comfyui_controlnet_aux/src/custom_controlnet_aux/util.py:44: UserWarning: Custom pressesor model path not set successfully.\n","  warnings.warn(\"Custom pressesor model path not set successfully.\")\n","/content/drive/MyDrive/Thesis/StableDiffusion/ComfyUI/Test/ComfyUI-master/custom_nodes/comfyui_controlnet_aux/src/custom_controlnet_aux/util.py:50: UserWarning: USE_SYMLINKS not set successfully. Using default value: False to download models.\n","  warnings.warn(\"USE_SYMLINKS not set successfully. Using default value: False to download models.\")\n","/content/drive/MyDrive/Thesis/StableDiffusion/ComfyUI/Test/ComfyUI-master/custom_nodes/comfyui_controlnet_aux/src/custom_controlnet_aux/util.py:59: UserWarning: custom temp dir not set successfully\n","  warnings.warn(f\"custom temp dir not set successfully\")\n","\u001b[36;20m[comfyui_controlnet_aux] | INFO -> Using ckpts path: /content/drive/.shortcut-targets-by-id/1iUIoDZT7_wu0slKT1AdgBSLWgWfdQNvc/Thesis/StableDiffusion/ComfyUI/Test/ComfyUI-master/custom_nodes/comfyui_controlnet_aux/ckpts\u001b[0m\n","\u001b[36;20m[comfyui_controlnet_aux] | INFO -> Using symlinks: False\u001b[0m\n","\u001b[36;20m[comfyui_controlnet_aux] | INFO -> Using ort providers: ['CUDAExecutionProvider', 'DirectMLExecutionProvider', 'OpenVINOExecutionProvider', 'ROCMExecutionProvider', 'CPUExecutionProvider', 'CoreMLExecutionProvider']\u001b[0m\n","/content/drive/MyDrive/Thesis/StableDiffusion/ComfyUI/Test/ComfyUI-master/custom_nodes/comfyui_controlnet_aux/node_wrappers/dwpose.py:26: UserWarning: DWPose: Onnxruntime not found or doesn't come with acceleration providers, switch to OpenCV with CPU device. DWPose might run very slowly\n","  warnings.warn(\"DWPose: Onnxruntime not found or doesn't come with acceleration providers, switch to OpenCV with CPU device. DWPose might run very slowly\")\n"]}]},{"cell_type":"code","source":["# @title Function\n","def get_value_at_index(obj: Union[Sequence, Mapping], index: int) -> Any:\n","    try:\n","        return obj[index]\n","    except KeyError:\n","        return obj[\"result\"][index]\n","\n","\n","def find_path(name: str, path: str = None) -> str:\n","    # If no path is given, use the current working directory\n","    if path is None:\n","        path = os.getcwd()\n","\n","    # Check if the current directory contains the name\n","    if name in os.listdir(path):\n","        path_name = os.path.join(path, name)\n","        print(f\"{name} found: {path_name}\")\n","        return path_name\n","\n","    # Get the parent directory\n","    parent_directory = os.path.dirname(path)\n","\n","    # If the parent directory is the same as the current directory, we've reached the root and stop the search\n","    if parent_directory == path:\n","        return None\n","\n","    # Recursively call the function with the parent directory\n","    return find_path(name, parent_directory)\n","\n","\n","def add_comfyui_directory_to_sys_path() -> None:\n","    comfyui_path = find_path(\"ComfyUI\")\n","    if comfyui_path is not None and os.path.isdir(comfyui_path):\n","        sys.path.append(comfyui_path)\n","        print(f\"'{comfyui_path}' added to sys.path\")\n","\n","\n","def load_extra_path_config(yaml_path):\n","    with open(yaml_path, 'r') as stream:\n","        config = yaml.safe_load(stream)\n","    for c in config:\n","        conf = config[c]\n","        if conf is None:\n","            continue\n","        base_path = None\n","        if \"base_path\" in conf:\n","            base_path = conf.pop(\"base_path\")\n","        for x in conf:\n","            for y in conf[x].split(\"\\n\"):\n","                if len(y) == 0:\n","                    continue\n","                full_path = y\n","                if base_path is not None:\n","                    full_path = os.path.join(base_path, full_path)\n","                logging.info(\"Adding extra search path {} {}\".format(x, full_path))\n","                folder_paths.add_model_folder_path(x, full_path)\n","\n","def add_extra_model_paths() -> None:\n","    extra_model_paths = find_path(\"extra_model_paths.yaml\")\n","\n","    if extra_model_paths is not None:\n","        load_extra_path_config(extra_model_paths)\n","    else:\n","        print(\"Could not find the extra_model_paths config file.\")\n","\n","def import_custom_nodes() -> None:\n","    # Creating a new event loop and setting it as the default loop\n","    loop = asyncio.new_event_loop()\n","    asyncio.set_event_loop(loop)\n","\n","    # Creating an instance of PromptServer with the loop\n","    server_instance = server.PromptServer(loop)\n","    execution.PromptQueue(server_instance)\n","\n","    # Initializing custom nodes\n","    init_extra_nodes()\n","\n","\n","add_comfyui_directory_to_sys_path()\n","add_extra_model_paths()\n","import_custom_nodes()"],"metadata":{"id":"sc-Ei7nbn-QH","colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","collapsed":true,"executionInfo":{"status":"ok","timestamp":1728553652951,"user_tz":-420,"elapsed":23953,"user":{"displayName":"Hung To","userId":"12784896086462809074"}},"outputId":"f76fad47-17c9-45f8-e772-d4b4127e017e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Could not find the extra_model_paths config file.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n","  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"]},{"output_type":"stream","name":"stdout","text":["------------------------------------------\n","\u001b[34mComfyroll Studio v1.76 : \u001b[92m 175 Nodes Loaded\u001b[0m\n","------------------------------------------\n","** For changes, please see patch notes at https://github.com/Suzie1/ComfyUI_Comfyroll_CustomNodes/blob/main/Patch_Notes.md\n","** For help, please see the wiki at https://github.com/Suzie1/ComfyUI_Comfyroll_CustomNodes/wiki\n","------------------------------------------\n","### Loading: ComfyUI-Impact-Pack (V7.5.2)\n","### Loading: ComfyUI-Impact-Pack (Subpack: V0.7)\n","[WARN] ComfyUI-Impact-Pack: `ComfyUI` or `ComfyUI-Manager` is an outdated version.\n","[Impact Pack] Wildcards loading done.\n","\u001b[92m[Allor]\u001b[0m: 0 nodes were overridden.\n","\u001b[92m[Allor]\u001b[0m: 12 modules were enabled.\n","\u001b[92m[Allor]\u001b[0m: 98 nodes were loaded.\n"]}]},{"cell_type":"code","source":["# @title Loading Resources\n","vaeloader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n","checkpointloadersimple = NODE_CLASS_MAPPINGS[\"CheckpointLoaderSimple\"]()\n","cliptextencode = NODE_CLASS_MAPPINGS[\"CLIPTextEncode\"]()\n","loadimage = NODE_CLASS_MAPPINGS[\"LoadImage\"]()\n","controlnetloader = NODE_CLASS_MAPPINGS[\"ControlNetLoader\"]()\n","ipadaptermodelloader = NODE_CLASS_MAPPINGS[\"IPAdapterModelLoader\"]()\n","clipvisionloader = NODE_CLASS_MAPPINGS[\"CLIPVisionLoader\"]()\n","samloader = NODE_CLASS_MAPPINGS[\"SAMLoader\"]()\n","ultralyticsdetectorprovider = NODE_CLASS_MAPPINGS[\"UltralyticsDetectorProvider\"]()\n","groundingdinomodelloader_segment_anything = NODE_CLASS_MAPPINGS[\"GroundingDinoModelLoader (segment anything)\"]()\n","groundingdinosamsegment_segment_anything = NODE_CLASS_MAPPINGS[\"GroundingDinoSAMSegment (segment anything)\"]()\n","imagescaletototalpixels = NODE_CLASS_MAPPINGS[\"ImageScaleToTotalPixels\"]()\n","getimagesize = NODE_CLASS_MAPPINGS[\"GetImageSize\"]()\n","emptylatentimage = NODE_CLASS_MAPPINGS[\"EmptyLatentImage\"]()\n","prepimageforclipvision = NODE_CLASS_MAPPINGS[\"PrepImageForClipVision\"]()\n","ipadapteradvanced = NODE_CLASS_MAPPINGS[\"IPAdapterAdvanced\"]()\n","freeu_v2 = NODE_CLASS_MAPPINGS[\"FreeU_V2\"]()\n","# dwpreprocessor = NODE_CLASS_MAPPINGS[\"DWPreprocessor\"]()\n","controlnetapplyadvanced = NODE_CLASS_MAPPINGS[\"ControlNetApplyAdvanced\"]()\n","cr_model_input_switch = NODE_CLASS_MAPPINGS[\"CR Model Input Switch\"]()\n","ksampleradvanced = NODE_CLASS_MAPPINGS[\"KSamplerAdvanced\"]()\n","nnlatentupscale = NODE_CLASS_MAPPINGS[\"NNLatentUpscale\"]()\n","ksampler = NODE_CLASS_MAPPINGS[\"KSampler\"]()\n","vaedecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n","bboxdetectorsegs = NODE_CLASS_MAPPINGS[\"BboxDetectorSEGS\"]()\n","samdetectorcombined = NODE_CLASS_MAPPINGS[\"SAMDetectorCombined\"]()\n","impactsegsandmask = NODE_CLASS_MAPPINGS[\"ImpactSegsAndMask\"]()\n","conditioningcombine = NODE_CLASS_MAPPINGS[\"ConditioningCombine\"]()\n","detailerforeachdebug = NODE_CLASS_MAPPINGS[\"DetailerForEachDebug\"]()\n","saveimage = NODE_CLASS_MAPPINGS[\"SaveImage\"]()\n","catvtonwrapper = NODE_CLASS_MAPPINGS[\"CatVTONWrapper\"]()\n","\n","vaeloader_8 = vaeloader.load_vae(vae_name=\"vae-ft-mse-840000-ema-pruned.safetensors\")\n","checkpointloadersimple_16 = checkpointloadersimple.load_checkpoint(ckpt_name=\"realdream.safetensors\")\n","controlnetloader_156 = controlnetloader.load_controlnet(control_net_name=\"control_v11p_sd15_openpose.pth\")\n","ipadaptermodelloader_256 = ipadaptermodelloader.load_ipadapter_model(ipadapter_file=\"ip-adapter-full-face_sd15.safetensors\")\n","clipvisionloader_257 = clipvisionloader.load_clip(clip_name=\"model.safetensors\")\n","ultralyticsdetectorprovider_266 = ultralyticsdetectorprovider.doit(model_name=\"bbox/face_yolov8m.pt\")\n","samloader_268 = samloader.load_model(model_name=\"sam_vit_b_01ec64.pth\", device_mode=\"Prefer GPU\")\n","cliptextencode_274 = cliptextencode.encode(text=\"a face\", clip=get_value_at_index(checkpointloadersimple_16, 1))\n","groundingdinomodelloader_segment_anything_306 = (groundingdinomodelloader_segment_anything.main(model_name=\"GroundingDINO_SwinT_OGC (694MB)\"))\n","samloader_307 = samloader.load_model(model_name=\"sam_vit_b_01ec64.pth\", device_mode=\"Prefer GPU\")\n","\n","bbox_detector=\"yolox_l.onnx\"\n","pose_estimator=\"dw-ll_ucoco_384.onnx\"\n","yolo_repo=\"yzd-v/DWPose\"\n","pose_repo=\"yzd-v/DWPose\"\n","DWPOSE_MODEL_NAME = \"yzd-v/DWPose\"\n","\n","model = DwposeDetector.from_pretrained(\n","        pose_repo,\n","        yolo_repo,\n","        det_filename=bbox_detector, pose_filename=pose_estimator,\n","        torchscript_device=model_management.get_torch_device()\n","    )\n","\n","class DWPose_Preprocessor:\n","    @classmethod\n","    def INPUT_TYPES(s):\n","        return define_preprocessor_inputs(\n","            detect_hand=INPUT.COMBO([\"enable\", \"disable\"]),\n","            detect_body=INPUT.COMBO([\"enable\", \"disable\"]),\n","            detect_face=INPUT.COMBO([\"enable\", \"disable\"]),\n","            resolution=INPUT.RESOLUTION(),\n","            bbox_detector=INPUT.COMBO(\n","                [\"yolox_l.torchscript.pt\", \"yolox_l.onnx\", \"yolo_nas_l_fp16.onnx\", \"yolo_nas_m_fp16.onnx\", \"yolo_nas_s_fp16.onnx\"],\n","                default=\"yolox_l.onnx\"\n","            ),\n","            pose_estimator=INPUT.COMBO(\n","                [\"dw-ll_ucoco_384_bs5.torchscript.pt\", \"dw-ll_ucoco_384.onnx\", \"dw-ll_ucoco.onnx\"],\n","                default=\"dw-ll_ucoco_384_bs5.torchscript.pt\"\n","            ),\n","            scale_stick_for_xinsr_cn=INPUT.COMBO([\"disable\", \"enable\"])\n","        )\n","\n","    RETURN_TYPES = (\"IMAGE\", \"POSE_KEYPOINT\")\n","    FUNCTION = \"estimate_pose\"\n","\n","    CATEGORY = \"ControlNet Preprocessors/Faces and Poses Estimators\"\n","\n","    def estimate_pose(self, image, detect_hand=\"enable\", detect_body=\"enable\", detect_face=\"enable\", resolution=512, model=None, scale_stick_for_xinsr_cn=\"disable\", **kwargs):\n","        detect_hand = detect_hand == \"enable\"\n","        detect_body = detect_body == \"enable\"\n","        detect_face = detect_face == \"enable\"\n","        scale_stick_for_xinsr_cn = scale_stick_for_xinsr_cn == \"enable\"\n","        self.openpose_dicts = []\n","        def func(image, **kwargs):\n","            pose_img, openpose_dict = model(image, **kwargs)\n","            self.openpose_dicts.append(openpose_dict)\n","            return pose_img\n","\n","        out = common_annotator_call(func, image, include_hand=detect_hand, include_face=detect_face, include_body=detect_body, image_and_json=True, resolution=resolution, xinsr_stick_scaling=scale_stick_for_xinsr_cn)\n","        del model\n","        return {\n","            'ui': { \"openpose_json\": [json.dumps(self.openpose_dicts, indent=4)] },\n","            \"result\": (out, self.openpose_dicts)\n","        }\n","\n","dwpreprocessor = DWPose_Preprocessor()\n","\n","class LoadImage:\n","    @classmethod\n","    def INPUT_TYPES(s):\n","        input_dir = folder_paths.get_input_directory()\n","        files = [f for f in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, f))]\n","        return {\"required\":\n","                    {\"image\": (sorted(files), {\"image_upload\": True})},\n","                }\n","\n","    CATEGORY = \"image\"\n","\n","    RETURN_TYPES = (\"IMAGE\", \"MASK\")\n","    FUNCTION = \"load_image\"\n","    def load_image(self, image):\n","        img = image\n","\n","        output_images = []\n","        output_masks = []\n","        w, h = None, None\n","\n","        excluded_formats = ['MPO']\n","\n","        for i in ImageSequence.Iterator(img):\n","            i = node_helpers.pillow(ImageOps.exif_transpose, i)\n","\n","            if i.mode == 'I':\n","                i = i.point(lambda i: i * (1 / 255))\n","            image = i.convert(\"RGB\")\n","\n","            if len(output_images) == 0:\n","                w = image.size[0]\n","                h = image.size[1]\n","\n","            if image.size[0] != w or image.size[1] != h:\n","                continue\n","\n","            image = np.array(image).astype(np.float32) / 255.0\n","            image = torch.from_numpy(image)[None,]\n","            if 'A' in i.getbands():\n","                mask = np.array(i.getchannel('A')).astype(np.float32) / 255.0\n","                mask = 1. - torch.from_numpy(mask)\n","            else:\n","                mask = torch.zeros((64,64), dtype=torch.float32, device=\"cpu\")\n","            output_images.append(image)\n","            output_masks.append(mask.unsqueeze(0))\n","\n","        if len(output_images) > 1 and img.format not in excluded_formats:\n","            output_image = torch.cat(output_images, dim=0)\n","            output_mask = torch.cat(output_masks, dim=0)\n","        else:\n","            output_image = output_images[0]\n","            output_mask = output_masks[0]\n","\n","        return (output_image, output_mask)\n","\n","    @classmethod\n","    def IS_CHANGED(s, image):\n","        image_path = folder_paths.get_annotated_filepath(image)\n","        m = hashlib.sha256()\n","        with open(image_path, 'rb') as f:\n","            m.update(f.read())\n","        return m.digest().hex()\n","\n","    @classmethod\n","    def VALIDATE_INPUTS(s, image):\n","        if not folder_paths.exists_annotated_filepath(image):\n","            return \"Invalid image file: {}\".format(image)\n","\n","        return True\n","\n","loadimage = LoadImage()"],"metadata":{"id":"trEtlivKoB_N","executionInfo":{"status":"ok","timestamp":1728553715347,"user_tz":-420,"elapsed":62399,"user":{"displayName":"Hung To","userId":"12784896086462809074"}},"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"cellView":"form","outputId":"98b65298-541f-4e60-b0ed-3fe51ff6a5dc"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/ultralytics/nn/tasks.py:837: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  ckpt = torch.load(file, map_location=\"cpu\")\n","/usr/local/lib/python3.10/dist-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(f)\n"]},{"output_type":"stream","name":"stdout","text":["Loads SAM model: /content/drive/.shortcut-targets-by-id/1iUIoDZT7_wu0slKT1AdgBSLWgWfdQNvc/Thesis/StableDiffusion/ComfyUI/Test/ComfyUI-master/models/sams/sam_vit_b_01ec64.pth (device:Prefer GPU)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"]},{"output_type":"stream","name":"stdout","text":["final text_encoder_type: bert-base-uncased\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/1iUIoDZT7_wu0slKT1AdgBSLWgWfdQNvc/Thesis/StableDiffusion/ComfyUI/Test/ComfyUI-master/custom_nodes/comfyui_segment_anything/node.py:127: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(\n"]},{"output_type":"stream","name":"stdout","text":["Loads SAM model: /content/drive/.shortcut-targets-by-id/1iUIoDZT7_wu0slKT1AdgBSLWgWfdQNvc/Thesis/StableDiffusion/ComfyUI/Test/ComfyUI-master/models/sams/sam_vit_b_01ec64.pth (device:Prefer GPU)\n","model_path is /content/drive/MyDrive/Thesis/StableDiffusion/ComfyUI/Test/ComfyUI-master/custom_nodes/comfyui_controlnet_aux/ckpts/yzd-v/DWPose/yolox_l.onnx\n","model_path is /content/drive/MyDrive/Thesis/StableDiffusion/ComfyUI/Test/ComfyUI-master/custom_nodes/comfyui_controlnet_aux/ckpts/yzd-v/DWPose/dw-ll_ucoco_384.onnx\n","\n","DWPose: Using yolox_l.onnx for bbox detection and dw-ll_ucoco_384.onnx for pose estimation\n","DWPose: Caching OpenCV DNN module yolox_l.onnx on cv2.DNN...\n","DWPose: Caching OpenCV DNN module dw-ll_ucoco_384.onnx on cv2.DNN...\n"]}]},{"cell_type":"code","source":["# @title Inference Code\n","def inference_code(face, pose, clothes):\n","    # Enter inference mode (turns off gradient calculations for memory optimization)\n","    with torch.inference_mode():\n","\n","        images = None\n","\n","        # Encode the negative prompt text into CLIP text embeddings\n","        # This is used to suppress unwanted features in the final image\n","        cliptextencode_18 = cliptextencode.encode(\n","            text=negative_prompt,  # The negative text prompt (what should be avoided in the image)\n","            clip=get_value_at_index(checkpointloadersimple_16, 1),  # Load the CLIP text encoder from the checkpoint\n","        )\n","\n","        # Encode the positive prompt text into CLIP text embeddings\n","        # This encourages the model to include features described by the positive prompt\n","        cliptextencode_30 = cliptextencode.encode(\n","            text=prompt,  # The positive text prompt (what should be included in the image)\n","            clip=get_value_at_index(checkpointloadersimple_16, 1),  # Load the CLIP text encoder from the checkpoint\n","        )\n","\n","        # Load the pose image (which may provide body positioning) from the specified path\n","        loadimage_155 = loadimage.load_image(image=pose)\n","\n","        # Load the face image, used to condition the generation on specific facial features\n","        loadimage_161 = loadimage.load_image(image=face)\n","\n","        # Load the clothes image, which may represent the outfit to be worn by the subject\n","        loadimage_304 = loadimage.load_image(image=clothes)\n","\n","        # Loop over the number of iterations (currently set to 1 iteration, meaning no real loop here)\n","        for q in range(1):\n","\n","            # Upscale the pose image to a total size of 0.5 megapixels using nearest neighbor scaling\n","            # This step adjusts the image resolution for further processing\n","            imagescaletototalpixels_168 = imagescaletototalpixels.upscale(\n","                upscale_method=\"nearest-exact\",  # Use nearest-neighbor scaling for the image\n","                megapixels=0.5,  # Set target resolution to 0.5 megapixels\n","                image=get_value_at_index(loadimage_155, 0),  # Load the first item from the pose image batch\n","            )\n","\n","            # Get the size (width, height) of the upscaled pose image for further use\n","            getimagesize_208 = getimagesize.get_size(\n","                image=get_value_at_index(imagescaletototalpixels_168, 0)  # Use the upscaled pose image\n","            )\n","\n","            # Generate an empty latent image with the same dimensions as the upscaled image\n","            # The latent image acts as a placeholder for generating the new image in latent space\n","            emptylatentimage_21 = emptylatentimage.generate(\n","                width=get_value_at_index(getimagesize_208, 0),  # Use the width of the upscaled image\n","                height=get_value_at_index(getimagesize_208, 1),  # Use the height of the upscaled image\n","                batch_size=1,  # Generate a single latent image\n","            )\n","\n","            # Prepare the face image for use with CLIP Vision by resizing and sharpening it\n","            # This step ensures the face image is compatible with the model’s expectations\n","            prepimageforclipvision_259 = prepimageforclipvision.prep_image(\n","                interpolation=\"LANCZOS\",  # Use Lanczos interpolation for resizing (high-quality method)\n","                crop_position=\"center\",  # Center the crop around the face\n","                sharpening=0,  # No additional sharpening is applied\n","                image=get_value_at_index(loadimage_161, 0),  # Load the first face image\n","            )\n","\n","            # Apply the IP-Adapter model to the prepared face image\n","            # The IP-Adapter adapts the input image by fusing visual and textual embeddings\n","            ipadapteradvanced_303 = ipadapteradvanced.apply_ipadapter(\n","                weight=1,  # Weight for the adaptation process (how strongly to apply this transformation)\n","                weight_type=\"linear\",  # Use a linear weighting scheme\n","                combine_embeds=\"concat\",  # Combine the embeddings by concatenation (text and image)\n","                start_at=0,  # Start adapting from the first layer of the model\n","                end_at=1,  # End adapting after one layer\n","                embeds_scaling=\"V only\",  # Only scale the embeddings from the Vision model (CLIP Vision)\n","                model=get_value_at_index(checkpointloadersimple_16, 0),  # Main generation model (from the checkpoint)\n","                ipadapter=get_value_at_index(ipadaptermodelloader_256, 0),  # IP-Adapter model\n","                image=get_value_at_index(prepimageforclipvision_259, 0),  # The prepared face image\n","                clip_vision=get_value_at_index(clipvisionloader_257, 0),  # The CLIP Vision model\n","            )\n","\n","            # Apply the FreeU-V2 model, which enhances the image using patch-based processing\n","            # This model applies transformations to specific parts of the image (patches) for refinement\n","            freeu_v2_252 = freeu_v2.patch(\n","                b1=1.3,  # Parameter controlling some aspect of patch processing\n","                b2=1.4,  # Another parameter for patch processing\n","                s1=0.9,  # Scaling factor for patches\n","                s2=0.2,  # Scaling factor for another stage of the patch process\n","                model=get_value_at_index(ipadapteradvanced_303, 0),  # Apply FreeU-V2 on the adapted model output\n","            )\n","\n","            # Estimate the human pose (body, face, hands) from the upscaled pose image\n","            # This provides key points for the body and helps condition the image generation process\n","            dwpreprocessor_238 = dwpreprocessor.estimate_pose(\n","                detect_hand=\"enable\",  # Enable detection of hand key points\n","                detect_body=\"enable\",  # Enable detection of body key points\n","                detect_face=\"enable\",  # Enable detection of face key points\n","                resolution=512,  # Set resolution for pose detection\n","                model=model,  # Use the provided model for pose estimation\n","                scale_stick_for_xinsr_cn=\"disable\",  # Disable scaling for stick figure models\n","                image=get_value_at_index(imagescaletototalpixels_168, 0),  # The upscaled pose image\n","            )\n","\n","            # Apply ControlNet for advanced conditioning of the image\n","            controlnetapplyadvanced_240 = controlnetapplyadvanced.apply_controlnet(\n","                strength=1, start_percent=0, end_percent=1,\n","                positive=get_value_at_index(cliptextencode_30, 0),\n","                negative=get_value_at_index(cliptextencode_18, 0),\n","                control_net=get_value_at_index(controlnetloader_156, 0),\n","                image=get_value_at_index(dwpreprocessor_238, 0)\n","            )\n","\n","            # Switch between two models for further image processing\n","            cr_model_input_switch_291 = cr_model_input_switch.switch(\n","                Input=2,  # Model 2 is selected\n","                model1=get_value_at_index(checkpointloadersimple_16, 0),\n","                model2=get_value_at_index(freeu_v2_252, 0)\n","            )\n","\n","            # Perform image sampling with advanced settings, applying noise and steps configuration\n","            ksampleradvanced_253 = ksampleradvanced.sample(\n","                add_noise=\"enable\", noise_seed=random.randint(1, 2**64), steps=20, cfg=5.5,\n","                sampler_name=\"heunpp2\", scheduler=\"karras\", start_at_step=0, end_at_step=10000,\n","                return_with_leftover_noise=\"disable\", model=get_value_at_index(cr_model_input_switch_291, 0),\n","                positive=get_value_at_index(controlnetapplyadvanced_240, 0),\n","                negative=get_value_at_index(controlnetapplyadvanced_240, 1),\n","                latent_image=get_value_at_index(emptylatentimage_21, 0)\n","            )\n","\n","            # Upscale the latent image using a neural network-based latent space upscaling\n","            nnlatentupscale_263 = nnlatentupscale.upscale(\n","                version=\"SD 1.x\", upscale=1.5, latent=get_value_at_index(ksampleradvanced_253, 0)\n","            )\n","\n","            # Perform image sampling again with different settings, using \"uni_pc\" sampler\n","            ksampler_176 = ksampler.sample(\n","                seed=random.randint(1, 2**64), steps=16, cfg=5.6,\n","                sampler_name=\"uni_pc\", scheduler=\"karras\", denoise=0.25,\n","                model=get_value_at_index(freeu_v2_252, 0),\n","                positive=get_value_at_index(controlnetapplyadvanced_240, 0),\n","                negative=get_value_at_index(controlnetapplyadvanced_240, 1),\n","                latent_image=get_value_at_index(nnlatentupscale_263, 0)\n","            )\n","\n","            # Upscale the latent image again for better resolution\n","            nnlatentupscale_298 = nnlatentupscale.upscale(\n","                version=\"SD 1.x\", upscale=1.5, latent=get_value_at_index(ksampler_176, 0)\n","            )\n","\n","            # Final sampling with DPM++ 2M sampler, which adds additional refinement to the image\n","            ksampler_297 = ksampler.sample(\n","                seed=random.randint(1, 2**64), steps=14, cfg=5.5,\n","                sampler_name=\"dpmpp_2m\", scheduler=\"karras\", denoise=0.35,\n","                model=get_value_at_index(freeu_v2_252, 0),\n","                positive=get_value_at_index(controlnetapplyadvanced_240, 0),\n","                negative=get_value_at_index(controlnetapplyadvanced_240, 1),\n","                latent_image=get_value_at_index(nnlatentupscale_298, 0)\n","            )\n","\n","            # Decode the final latent representation into an actual image using VAE\n","            vaedecode_301 = vaedecode.decode(\n","                samples=get_value_at_index(ksampler_297, 0),\n","                vae=get_value_at_index(vaeloader_8, 0)\n","            )\n","\n","            # Detect bounding boxes in the decoded image for further segmentation using a threshold and dilation\n","            bboxdetectorsegs_267 = bboxdetectorsegs.doit(\n","                threshold=0.52, dilation=10, crop_factor=1.2, drop_size=10, labels=\"all\",\n","                bbox_detector=get_value_at_index(ultralyticsdetectorprovider_266, 0),\n","                image=get_value_at_index(vaedecode_301, 0)\n","            )\n","\n","            # Use SAM model to generate detailed segmentations based on detected bounding boxes and masks\n","            samdetectorcombined_269 = samdetectorcombined.doit(\n","                detection_hint=\"mask-points\", dilation=0, threshold=0.94, bbox_expansion=0,\n","                mask_hint_threshold=0.7, mask_hint_use_negative=\"False\",\n","                sam_model=get_value_at_index(samloader_268, 0),\n","                segs=get_value_at_index(bboxdetectorsegs_267, 0),\n","                image=get_value_at_index(vaedecode_301, 0)\n","            )\n","\n","            # Combine the detected bounding boxes and mask to create impact segments for the next steps\n","            impactsegsandmask_278 = impactsegsandmask.doit(\n","                segs=get_value_at_index(bboxdetectorsegs_267, 0),\n","                mask=get_value_at_index(samdetectorcombined_269, 0)\n","            )\n","\n","            # Combine different conditioning inputs (from text and ControlNet) for further fine-tuning\n","            conditioningcombine_275 = conditioningcombine.combine(\n","                conditioning_1=get_value_at_index(cliptextencode_274, 0),\n","                conditioning_2=get_value_at_index(controlnetapplyadvanced_240, 0)\n","            )\n","\n","            # Apply additional refinement to the generated image by using a debug detailer, applying noise, and inpainting\n","            detailerforeachdebug_270 = detailerforeachdebug.doit(\n","                guide_size=1024, guide_size_for=False, max_size=1024, seed=random.randint(1, 2**64),\n","                steps=16, cfg=8.5, sampler_name=\"ddpm\", scheduler=\"karras\", denoise=0.3,\n","                feather=6, noise_mask=True, force_inpaint=True, wildcard=\"\", cycle=1,\n","                inpaint_model=False, noise_mask_feather=20,\n","                image=get_value_at_index(vaedecode_301, 0),\n","                segs=get_value_at_index(impactsegsandmask_278, 0),\n","                model=get_value_at_index(checkpointloadersimple_16, 0),\n","                clip=get_value_at_index(checkpointloadersimple_16, 1),\n","                vae=get_value_at_index(vaeloader_8, 0),\n","                positive=get_value_at_index(conditioningcombine_275, 0),\n","                negative=get_value_at_index(controlnetapplyadvanced_240, 1)\n","            )\n","\n","            # Change Clothes Operation\n","            # Using Grounding DINO and SAM to segment specific areas based on a prompt and apply new clothes\n","            groundingdinosamsegment_segment_anything_305 = groundingdinosamsegment_segment_anything.main(\n","                prompt=f\"{replace_prompt}\", threshold=0.3,\n","                sam_model=get_value_at_index(samloader_307, 0),\n","                grounding_dino_model=get_value_at_index(groundingdinomodelloader_segment_anything_306, 0),\n","                image=get_value_at_index(detailerforeachdebug_270, 0)\n","            )\n","\n","            # Use CAT-VTON to change the clothes based on the detected segmentation mask and a reference image\n","            catvtonwrapper_308 = catvtonwrapper.catvton(\n","                mask_grow=25, mixed_precision=\"fp16\", seed=random.randint(1, 2**64), steps=40, cfg=2.5,\n","                image=get_value_at_index(detailerforeachdebug_270, 0),\n","                mask=get_value_at_index(groundingdinosamsegment_segment_anything_305, 1),\n","                refer_image=get_value_at_index(loadimage_304, 0)\n","            )\n","\n","            # Save the intermediate image after the debug detailing process\n","            saveimage_277 = saveimage.save_images(\n","                filename_prefix=\"Reposer_S2_Facefix\",\n","                images=get_value_at_index(detailerforeachdebug_270, 0)\n","            )\n","\n","            # Save the final image after applying the clothes change\n","            saveimage_309 = saveimage.save_images(\n","                filename_prefix=\"ChangeClothes\",\n","                images=get_value_at_index(catvtonwrapper_308, 0)\n","            )\n","\n","            return get_value_at_index(catvtonwrapper_308, 0)"],"metadata":{"id":"DtgGuYv-p5nI","executionInfo":{"status":"ok","timestamp":1728553715348,"user_tz":-420,"elapsed":18,"user":{"displayName":"Hung To","userId":"12784896086462809074"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# @title Gradio Interface\n","\n","import gradio as gr\n","\n","prompt = \"A beautiful woman wearing clothes, 8k uhd, dslr, soft lighting, high quality, film grain\"\n","negative_prompt = \"naked, nsfw, drab, colourless, low-quality,blurred,jpeg artifacts,cropped image,out of shot,cartoon,cg,comic,drawing,bad art,bad artist,bad fan art,CGI,grainy,kitsch,lazy art,less creative,lowres,noise,bad composition,mutated body parts,blurry image,disfigured,bad anatomy,deformed body features,double head,double figure,double body,extra fingers,mutated hands,poorly drawn hands,mutation,deformed,dehydrated,bad proportions,extra limbs,cloned face,gross proportions,malformed limbs,missing arms,missing legs,extra arms,extra legs,fused fingers,too many fingers,long neck,disconnected head,malformed hands,huge calf,fused hand,disappearing arms,disappearing thigh,disappearing calf,disappearing legs,abnormal eye proportion,Abnormal legs,abnormal feet,abnormal fingers,painting,crayon,graphite,deformed,mutated,mutation,long body,bad body,fused fingers,missing fingers,bad hands,disfigured,too many fingers,logo,text,watermark.\"\n","replace_prompt = \"clothes\"\n","\n","# # Define the Gradio interface\n","def generate_image(face, clothes, pose):\n","    # Call the inference function to process the images\n","    generated_image = inference_code(face, clothes, pose)\n","    image_tensor = generated_image[0].squeeze(0)\n","    image_tensor = image_tensor * 255.0\n","    image_np = image_tensor.numpy().astype(np.uint8)\n","    image_pil = Image.fromarray(image_np)\n","    return image_pil\n","\n","\n","# Define Gradio layout\n","with gr.Blocks() as demo:\n","    gr.Markdown(\"# Image Generation Interface\")\n","    gr.Markdown(\"Upload three images: face, clothes, and pose, then click generate.\")\n","\n","    # Row layout for input images\n","    with gr.Row():\n","        face_input = gr.Image(type=\"pil\", label=\"Face Image\")\n","        clothes_input = gr.Image(type=\"pil\", label=\"Clothes Image\")\n","        pose_input = gr.Image(type=\"pil\", label=\"Pose Image\")\n","\n","    # Center output image at the bottom\n","    with gr.Row():\n","        generate_button = gr.Button(\"Generate\")\n","\n","    # Display the output in a single row at the center bottom\n","    with gr.Row():\n","        output_image = gr.Image(type=\"pil\", label=\"Generated Image\")\n","\n","    # Define what happens when the generate button is clicked\n","    generate_button.click(generate_image, inputs=[face_input, clothes_input, pose_input], outputs=output_image)\n","\n","# Launch the interface\n","demo.launch(debug=True, share=True)"],"metadata":{"id":"cnsUfg-4pKMt","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["ea36acf91ce249e2b27434e2072f4b30","421392369d6840bc9dc1c9da6a73ee96","61b67ef9069f4d569d5cf3f03051d22f","45e53a0d0b044ad8abedf266f52735ac","2eb6900887ce4e67a511587a6d4ee6e7","e3d755fc5c224bdfa8edf1d37883edb2","5eb816407d594d1fb7d800510b542ca5","1b56e9c3c99b4290a98247d9aa999450","859b503102bd44aa9fdf1490393d1d34","f959834e20474bf4b7629b893fbdb024","314787ae608546c4b1d95e0bf655a524","8a236713591141a2a585265b945f1120","96045f66b8ad412ebac5189e30ef337b","d34640a2b79d4215b3eb04e58463e013","19540eaab9584a94b126aeb3ffb8c0d7","621e43a66a414c1784d11dd0e157fc99","643cc5c7bcca471999a8c9d7bb3e14a6","6886ef870ef24bd6b6419cee0df6f0d2","93cd5b796989485ab7198e2a9cd17296","2512eefc125d4a308948c0c81389bf33","ca2c15fc95f340199a5808db6cc353c4","c267ca2be886453ab4aa0cfcb699718d","0394be08d79248188382c08d138d471e","41abb300f21d43c99ce2ef9170cacd07","26487ec5c8d947e09a78fb90797f8275","b641419419bd4667a808f6e9b7792794","80968e2b75e541bebece82e939827e47","8114a0f9971b470a90580ac079e1b0b2","4376b003c0e94e28ae5984c12ba7f70a","6939c645a86a44788a62148f1dc006e0","eab2c6a995d14c039006480c7cadfc40","2efc4055b88b4e70b58815b2319b3a19","abf697efbff94eb59529d77c58ebfff0","42d014777de043d6880bb86dfb06f962","af1f8f4ea20e4b419a970c287379fbc4","8926e3b30d694587a25b014a38bbc3e0","17ee25ca3db94d0b87358a7d2f08741c","882e44afe0b34fbf869a5aa561eaf9da","c0857450521942df96df5c74184f6929","e5064ec8957945c8a2fc50a8da66de11","130e8f5d2a3a4a4f979fe900ffb5f830","2b6e2fb5ad38420c8e796d61a6cbd6c2","175ab3e5c64d46d79cd17b310a6c93f8","8605083047414c1ba01660eeb11a8220"]},"outputId":"28c46391-105c-4ca6-edf6-bb25bbf074f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","* Running on public URL: https://24fbf7792917e4a619.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://24fbf7792917e4a619.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["DWPose: Bbox 4031.40ms\n","DWPose: Pose 752.32ms on 1 people\n","\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/15 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea36acf91ce249e2b27434e2072f4b30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/13 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a236713591141a2a585265b945f1120"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/10 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0394be08d79248188382c08d138d471e"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","0: 640x640 1 face, 17.1ms\n","Speed: 32.4ms preprocess, 17.1ms inference, 156.2ms postprocess per image at shape (1, 3, 640, 640)\n","Detailer: segment upscale for ((284.1562, 90.221306)) | crop region (340, 108) x 3.0124222888728003 -> (1024, 325)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/16 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42d014777de043d6880bb86dfb06f962"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1126: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","/content/drive/.shortcut-targets-by-id/1iUIoDZT7_wu0slKT1AdgBSLWgWfdQNvc/Thesis/StableDiffusion/ComfyUI/Test/ComfyUI-master/custom_nodes/comfyui_segment_anything/local_groundingdino/models/GroundingDINO/transformer.py:862: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=False):\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"vMS5MSTzqVef"},"execution_count":null,"outputs":[]}]}